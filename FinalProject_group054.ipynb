{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will Your Car Hold Its Value?\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Taylor Drennen\n",
    "- Jaime Altamirano-Ramirez\n",
    "- Geoffrey Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Our project’s main objective is to create a model that predicts whether the used car will hold its value based on the year, model, mileage, and maker of the car. We used a dataset that contains used car prices along with the features stated above to train the model. In addition, to determine whether the used cars listed in the data held their value, we determined the standard depreciation threshold based on Manufacturer Suggested Retail Price (MSRP) extracted from the CARFAX website and create a new feature of the data. The model was trained using a multivariate dataset classification algorithm such as a Support-Vector Machine (SVM) and aDecision Tree and used cross-validation to measure the performance of this model. \n",
    "\n",
    "- major results you came up with (mention how results are measured) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In the modern society, it is undeniable that the car is essial asset to live. However, it is also considered luxury item due to its expense. Not everyone can afford brand new car; therefor, many people opt out for used car instead. During pandemic, due to low supply of semiconductor chips, the car production declined. As a result, there were not enough new cars for people to purchase, forcing car buyers into used car market  instead <a name=\"fried\"></a>[<sup>[3]</sup>](#fried).  SInce there is always a demand for used car, there is also demand for people to sell their cars. The common concern people may have when they sell their cars is whether they are getting good deal out of it. That is the question our model will be answering. \n",
    "\n",
    "In the past, there were several other projects with purpose of price prediction of Used Cars. In 2021, IEEE published their used car prediction model. Many of these project focuses on price prediction for seller. The used car dealer spend their time and effort on the market to assess used vehicles.They check the vehicle's model, year of production, condition, and mileage, among others <a name=\"ieee\"></a>[<sup>[4]</sup>](#ieee). Thus, the used car price prediction model typically used those feature to predict the price. \n",
    "\n",
    "This project also go into price prediction just like IEEE project. However, we will be evaluating whether the price is fair. To evaluate this, we wil lexamine car depreciation rate. According to Carmax,  the value of a new vehicle drops by about 20% in the first year of ownership and over the next four years, you can expect your car to lose roughly 15% of its value each year <a name=\"carmax\"></a>[<sup>[5]</sup>](#carmax). Based on this fact, we can calculate standard depreciation threshold that decides whether the used car price is following the standard of depreciation rate and if not, it means that car did not hold it’s value. This will allow seller to see if they are selling the car for rightful price. In addition, buyer can see if they are paying the fair price. Our goal is for this model to benefit sellers, buyers, and car manufacturers in the used cars market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we aim to solve is whether a car will hold its market value at the time of selling. To elaborate, we plan on creating a reliable algorithm that can determine whether a car is likely to be sold at or above its depreciation value at the point of sale. With car prices nearing a record high, the desire to know whether a used car will hold its value is an important decision car owners are having to make. Given that there are a lot of variables that come into play when selling a car (e.g.make, model, mileage, location, year, condition, etc), and that some variables may hold more weight than others (such as the year of a car being a bigger factor than its mileage), we intend to compare a few models while optimizing our hyperparameters. We plan on turning our string data, such as location, into numerical data to align with the bulk of our expected variables which will be float types for our code. On a separate note, the global automobile market is expected to continue rising and the compound annual growth rate is expected to be 3.7% from 2020-to 2030 <a name=\"renub\"></a>[<sup>[1]</sup>](#renub) . Therefore, more used cars will be entering the market and allow for future data collection and analysis to be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "\n",
    "#### Used car price dataset\n",
    "https://www.kaggle.com/datasets/harikrishnareddyb/used-car-price-predictions/code\n",
    "- 8 Variables\n",
    "- 852122 Observations\n",
    "- May take out columns that we are not using and check for any Nan Variable\n",
    "- Need to convert a string variable to an integer through one-hot encoding \n",
    "\n",
    "#### Manufacturer Suggested Retail Price (MSRP)\n",
    "https://www.cars.com/research/{make}-{model}-{year}\n",
    "- Create a new dataset by collecting the MSRP of all vehicles in the previous dataset from the website linked above\n",
    "- Retrieve MSRP from the website\n",
    "- Model, year, and make is provided by the previous dataset\n",
    "\n",
    "#### Data Handling\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "For the purpose of this project, we will need a multivariate dataset classification model. The final model should take in the year, model, maker, and mileage of the car and determine if it will hold its value or in other words if the predicted price would be above the standard depreciation threshold.  As a group, we decided to compare the SVM and KNN algorithms to determine the best model for this project. \n",
    " \n",
    "#### Support-Vector Machine (SVM)\n",
    "\n",
    "SVM is a supervised learning technique; thus we will be training this model with the dataset.  We will be finding hyperplanes that divide the data into two clusters that determine whether the predicting datapoint holds value or not. SVM uses a kernel function to help calculate hyperplanes. This allows us to have a smoother calculation for the multidimensional dataset. For this project, we will use a linear kernel.  This model will be implemented using the SVM function from the sklearn library. \n",
    "\n",
    "To optimize this model, we will be changing the margin. The margin is the distance between the hyperplane and the nearest data, also known as the supporting vector. For the purpose of optimization, we want to maximize the margin; therefore, different weight vectors can be considered during evaluation.\n",
    "\n",
    "#### Decision Tree\n",
    "\n",
    "\n",
    "\n",
    "#### Required Libraries\n",
    "- numpy\n",
    "- requests\n",
    "- bs4\n",
    "- tqdm\n",
    "- black\n",
    "- ipykernel\n",
    "- pandas\n",
    "- matplotlib\n",
    "- isort\n",
    "- pylint\n",
    "- jinja2\n",
    "- sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using simple cross-validation to evaluate our model performance. We will divide the randomized data into a training set and a validation set.  The output of the model that was trained will be compared with the validation set to get the confusion matrix. The final score that determines the performance of our model will be given by the F1 score to minimize false negatives. F1 score will be used for model selection, and ROC curve will be used to see overall performance of our model.\n",
    "\n",
    "##### Accuracy: The ability of a model to accurately predict.\n",
    "$Accuray = \\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "##### Recall/Sensitivity:  The ability of a model to find all the relevant cases within a data set.\n",
    "$Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "##### Precision:  The ability of a model to identify only the relevant data points.\n",
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "##### F1 Score: optimal blend of precision and recall \n",
    "$F1 = \\frac{2*Recall*Precision}{Sensitivity + Recall}$\n",
    "\n",
    "#### Receiver Operating Characteristic(ROC) curve\n",
    "There are two parameters of the ROC Curve: \\\n",
    "\\\n",
    "$True Positive Rate(TPR) = Recall = \\frac{TP}{TP+FN}$\\\n",
    "$False Positive Rate (FPR) = \\frac{FP}{FP+TN}$\n",
    "\n",
    "ROC curve plots TPR vs FPR at different classification thresholds, in our cade different hyperparameters. It illustrates the performance of a classification model. To Evaluate the performance, we will look at Area Under the ROC Curve (AUC). AUC can be interpreted as the probability of our model ranking a random positive example more highly than a random negative example. The Higher the AUC, the better the model is at distinguishing between classes. ROC curve allows us to see how much the model is capable of distinguishing between classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -r requirements.txt\n",
    "\n",
    "from data_cleaning import clean_dataframe, append_msrp, append_retains_value, get_expected_value\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleaning takes ~20mins, so we try to avoid repeating it\n",
    "try:\n",
    "    df = pd.read_csv(\"true_car_cleaned.csv\", index_col=0)\n",
    "except FileNotFoundError:\n",
    "    # source: https://www.kaggle.com/datasets/harikrishnareddyb/used-car-price-predictions\n",
    "    df = pd.read_csv(\"true_car_listings.csv\")\n",
    "    df = clean_dataframe(df)\n",
    "    df = append_msrp(df)\n",
    "    df = append_retains_value(df)\n",
    "    df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the variable that will eventually become our class, `retains_value`, has very little correlation to `MSRP` and `Mileage`. This may suggest more of a connection between Make, Model, and specific model-years (since they aren't featured in this numerical-only matrix).\n",
    "\n",
    "`expected_value` is strongly correlated to `Price`, meaning on average our metric of depreciation mostly holds true. However, with this machine learning model we hope to further granularize this and give a better understanding of how different features affect how value is held (or not held)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('retains_value', axis=1)\n",
    "y = df['retains_value']\n",
    "\n",
    "numeric_features = ['Year', 'Mileage', 'MSRP']\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "categorical_features = ['Make', 'Model']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "# classifier = LinearSVC(verbose=1, dual=False)\n",
    "# classifier = SVC(verbose=1, kernel='linear')\n",
    "\n",
    "classifier = tree.DecisionTreeClassifier(min_impurity_decrease=0.0002)\n",
    "\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [0, 0.0001, 0.001, 0.01, 0.1]\n",
    "params = np.linspace(0, 0.001, 20)\n",
    "scores = []\n",
    "for param in params:\n",
    "    # print(f\"min_impurity_decrease={param}\")\n",
    "    classifier = tree.DecisionTreeClassifier(min_impurity_decrease=param)\n",
    "\n",
    "    clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    # print(score)\n",
    "    scores.append(score)\n",
    "\n",
    "best_score = max(scores)\n",
    "best_param = params[scores.index(best_score)]\n",
    "print(f\"Best parameter={best_param} yields score {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Minimum Impurity Decrease  | Score|\n",
    "|---|---|\n",
    "| 0  | 0.7268  | \n",
    "| 0.0001 |0.76915  | \n",
    "| 0.001 |0.69285  |\n",
    "| 0.01 | 0.55105  |\n",
    "| 0.1 | 0.50345 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(params, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.predict_proba(X_test)[:,1]\n",
    "fps = []\n",
    "tps = []\n",
    "for threshold in np.linspace(0,1,1000):\n",
    "    y_pred = scores > threshold\n",
    "    true_negatives, false_positives, false_negatives, true_positives = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fps.append(false_positives / (false_positives + true_negatives)) # add false positive rate\n",
    "    tps.append(true_positives / (true_positives + false_negatives)) # add true positive rate\n",
    "\n",
    "fpr = fps\n",
    "tpr = tps\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, scores)\n",
    "# plotting the ROC curve\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "print('Area under the Receiver Operating Characteristic curve:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree vs SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "The data we are retrieving from Kaggle contains attributes of the car and the value it was sold for. Given that we are not collecting or using personal information of individuals, we do not have to worry about informed consent of individuals. If we were to collect further data, we would discard any identifiable information from the individual or car- such as vehicle identification number. \n",
    "\n",
    "Although not yet apparent, we do speculate that there could be some bias towards the data collection for specific regions since more than 20% of the data collected comes from California and Texas, but this could be due to these two states being the most populated in the United States <a name=\"uscb\"></a>[<sup>[2]</sup>](#uscb). Thus, we plan on using the state location of a vehicle as an independent variable but also may experiment with stratified k-fold cross validation. \n",
    "\n",
    "Although the model may be used to price hike a car that would otherwise be sold at a lower value or low-ball a seller who would otherwise sell the car at a higher price, we believe that the accurate representation of whether a used car will hold its value is important for such decision-making. There could be a more extreme case where the brand of vehicles can be looked down upon after noting that the car value does not hold. In such a situation, we would make a clear statement that the maker of the car is not the sole defining factor and that important variables, like mileage and year, play a large role in the value of cars. Moreover, we would hope to further develop the model by potentially weighing variables differently and observing learning curves to determine if the model would lead to more accurate results. \n",
    "\n",
    "As of now, we have talks of testing and monitoring for a concept drift but are unclear as to how we would implement it. After solidifying our model, we will further explain the limitations, shortcomings, and biases that exist in our model.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"renub\"></a>1.[^](#renub): Global Automotive Market, Growth & Forecast, Impact of Coronavirus, Industry Trends, By Region, Opportunity Company Analysis. 2021. Research and Markets, https://www.researchandmarkets.com/reports/5447681/global-automotive-market-growth-and-forecast?utm_source=BW&utm_medium=PressRelease&utm_code=9r6nph&utm_campaign=1603404+-+Global+Automotive+Market%3a+COVID-19%2c+Growth+%26+Forecast+2020-2030&utm_exec=joca. Accessed 20th May 2022.<br> \n",
    "<a name=\"uscb\"></a>2.[^](#uscb): U.S. Census Bureau quickfacts: Idaho. (n.d.). Retrieved May 21, 2022, from https://www.census.gov/quickfacts/geo/chart/ID/PST045221<br>\n",
    "<a name=\"fried\"></a>3.[^](#fried): Fried, Carla, et al. “Dealers Might Not Profit from Soaring Used-Car Prices.” UCLA Anderson Review, UCLA Anderson Review, 16 Feb. 2022, https://anderson-review.ucla.edu/dealers-might-not-profit-from-soaring-used-car-prices/.  <br>\n",
    "<a name=\"ieee\"></a>4.[^](#ieee):Jin, Chuyang. “Price Prediction of Used Cars Using Machine Learning.” IEEE Xplore, IEEE, 8 Feb. 2022, https://ieeexplore.ieee.org/document/9696839. <br>\n",
    "<a name=\"carmax\"></a>5.[^](#carmax): Popely, Rick. “Car Depreciation: How Much It Costs You.” CARFAX, CARFAX, 26 Apr. 2021, https://www.carfax.com/blog/car-depreciation. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
